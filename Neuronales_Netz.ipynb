{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren, aller benötigten Packages und einlesen der Trainings-, Test- und Überprüfungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datensatz = pd.read_csv(\"./foot-traffic-wue/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten nach Fehlern und unrelevanten Daten bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datensatz_f = datensatz[\n",
    "    ~datensatz['incidents'].isin(['laser_failure'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markierung der Feiertage im Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Feiertage\n",
    "feiertage = [\n",
    "    '2019-04-19', '2019-04-22', '2019-05-01', '2019-05-30', '2019-06-10', '2019-06-20', '2019-08-15',\n",
    "    '2019-10-03', '2019-11-01', '2019-12-25', '2019-12-26', '2020-01-01', '2020-01-06', '2020-04-10',\n",
    "    '2020-04-13', '2020-05-01', '2020-05-21', '2020-06-01', '2020-06-11', '2020-08-15', '2020-10-03',\n",
    "    '2020-11-01', '2020-12-25', '2020-12-26', '2021-01-01', '2021-01-06', '2021-04-02', '2021-04-05',\n",
    "    '2021-05-01', '2021-05-13', '2021-05-24', '2021-06-03', '2021-08-15', '2021-10-03', '2021-11-01',\n",
    "    '2021-12-25', '2021-12-26', '2022-01-01', '2022-01-06', '2022-04-15', '2022-04-18', '2022-05-01',\n",
    "    '2022-05-26', '2022-06-06', '2022-06-16', '2022-08-15', '2022-10-03', '2022-11-01', '2022-12-25',\n",
    "    '2022-12-26', '2023-01-01', '2023-01-06', '2023-04-07', '2023-04-10', '2023-05-01', '2023-05-18',\n",
    "    '2023-05-29', '2023-06-08', '2023-08-15', '2023-10-03', '2023-11-01', '2023-12-25', '2023-12-26',\n",
    "    '2024-01-01', '2024-01-06', '2024-03-29', '2024-04-01', '2024-05-01', '2024-05-09', '2024-05-20',\n",
    "    '2024-05-30', '2024-08-15'\n",
    "]\n",
    "\n",
    "# Konvertiere die Feiertage zu einem Set für eine schnellere Suche\n",
    "feiertage_set = set(feiertage)\n",
    "\n",
    "# Erstelle die neue Spalte \"is_feiertag\"\n",
    "datensatz_f['is_feiertag'] = datensatz_f['date'].apply(lambda x: 1 if x in feiertage_set else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilung der Spalte date in year, month und day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere die Spalte 'date' in ein Datetime-Format\n",
    "datensatz_f['date'] = pd.to_datetime(datensatz_f['date'])\n",
    "\n",
    "# Zerlege die Spalte in 'year', 'month' und 'day'\n",
    "datensatz_f['year'] = datensatz_f['date'].dt.year\n",
    "datensatz_f['month'] = datensatz_f['date'].dt.month\n",
    "datensatz_f['day'] = datensatz_f['date'].dt.day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusammenführen der Wetterbedingungen, welche in Tag und Nacht unterteilt sind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Werte zusammenführen\n",
    "datensatz_f['weather_condition'] = datensatz_f['weather_condition'].replace({\n",
    "    'partly-cloudy-day': 'partly-cloudy',\n",
    "    'partly-cloudy-night': 'partly-cloudy',\n",
    "    'clear-day': 'clear',\n",
    "    'clear-night': 'clear'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufstellen des Modells mithilfe eines neuronalen Netzes (Standardkategorien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import initializers\n",
    "\n",
    "# Eingangs- und Zielvariablen definieren\n",
    "feature_columns = [\"streetname\", \"hour\", \"weekday\", \"incidents\", \"weather_condition\", \"temperature\", \"year\", \"month\", \"day\"]\n",
    "target_columns = [\"n_pedestrians\", \"n_pedestrians_towards\", \"n_pedestrians_away\"]\n",
    "\n",
    "\n",
    "# Encoder anlegen und an Daten anpassen\n",
    "def fit_preprocess(X: pd.DataFrame, ordinal_features: List[str] | None, onehot_features: List[str] | None, numerical_features: List[str] | None):\n",
    "\tX = X.copy()\n",
    "\n",
    "\tordinal_encoder = None\n",
    "\tif ordinal_features is not None and ordinal_features:\n",
    "\t\tordinal_encoder = OrdinalEncoder()\n",
    "\t\tX[ordinal_features] = ordinal_encoder.fit_transform(X[ordinal_features])\n",
    "\n",
    "\tonehot_encoder = None\n",
    "\tif onehot_features is not None and onehot_features:\n",
    "\t\tonehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\t\tencoded = onehot_encoder.fit_transform(X[onehot_features])\n",
    "\t\tencoded_onehot_columns = onehot_encoder.get_feature_names_out()\n",
    "\t\tX[encoded_onehot_columns] = pd.DataFrame(encoded, columns=encoded_onehot_columns)\n",
    "\n",
    "\tnumerical_encoder = None\n",
    "\tif numerical_features is not None and numerical_features:\n",
    "\t\tnumerical_encoder = StandardScaler()\n",
    "\t\tX[numerical_features] = numerical_encoder.fit_transform(X[numerical_features])\n",
    "\n",
    "\treturn ordinal_encoder, onehot_encoder, numerical_encoder\n",
    "\n",
    "\n",
    "# Codiert die Messkategorien\n",
    "def preprocess(X: pd.DataFrame, \n",
    "\t\t\t   ordinal_features, ordinal_encoder: OrdinalEncoder, \n",
    "\t\t\t   onehot_features, onehot_encoder: OneHotEncoder, \n",
    "\t\t\t   numerical_features: List[str] | None, numerical_encoder: StandardScaler):\n",
    "\tX = X.copy()\n",
    "\tfeatures = []\n",
    "\n",
    "\tif ordinal_encoder is not None:\n",
    "\t\tX[ordinal_features] = ordinal_encoder.transform(X[ordinal_features])\n",
    "\t\tfeatures.extend(ordinal_features)\n",
    "\n",
    "\tif onehot_encoder is not None:\n",
    "\t\tencoded = onehot_encoder.transform(X[onehot_features])\n",
    "\t\tencoded_onehot_columns = onehot_encoder.get_feature_names_out()\n",
    "\t\tX[encoded_onehot_columns] = pd.DataFrame(encoded, columns=encoded_onehot_columns)\n",
    "\t\tfeatures.extend(encoded_onehot_columns)\n",
    "\n",
    "\tif numerical_features is not None:\n",
    "\t\tX[numerical_features] = numerical_encoder.transform(X[numerical_features])\n",
    "\t\tfeatures.extend(numerical_features)\n",
    "\n",
    "\treturn X[features]\n",
    "\n",
    "\n",
    "# Kategorische und numerische Features den Codierungsverfahren zuordnen\n",
    "numerical_features = [\"year\", \"temperature\"]\n",
    "ordinal_features = [\"weather_condition\", \"incidents\"]\n",
    "onehot_features = [\"streetname\", \"weekday\", \"month\", \"hour\",  \"day\"]\n",
    "\n",
    "\n",
    "# Aufstellen der Encoder für den Datensatz\n",
    "ordinal_encoder, onehot_encoder, numerical_encoder = fit_preprocess(datensatz_f, ordinal_features=ordinal_features,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t onehot_features=onehot_features,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t numerical_features=numerical_features)\n",
    "\n",
    "\n",
    "# Daten vorverarbeiten (codieren)\n",
    "X = preprocess(datensatz_f[feature_columns],\n",
    "\t\t\t\t\tordinal_features=ordinal_features,\n",
    "\t\t\t\t\tordinal_encoder=ordinal_encoder,\n",
    "\t\t\t\t\tonehot_features=onehot_features,\n",
    "\t\t\t\t\tonehot_encoder=onehot_encoder,\n",
    "\t\t\t\t\tnumerical_features=numerical_features,\n",
    "\t\t\t\t\tnumerical_encoder=numerical_encoder)\n",
    "y = datensatz_f[target_columns]\n",
    "\n",
    "\n",
    "\n",
    "# Neuronales Netzwerk definieren\n",
    "def build_NN(X, y, n_nodes, n_hidden_layers, dropout):\n",
    "\t# Input Layer anlegen\n",
    "\tinput = layers.Input(shape=(X.shape[1],), name=\"input\")\n",
    "\n",
    "\t# Hidden Layer anlegen mit möglichem Dropout\n",
    "\ttmp = input\n",
    "\tfor i in range(n_hidden_layers):\n",
    "\t\ttmp = layers.Dense(n_nodes, activation=\"relu\", name=f\"hidden_{i}\")(tmp)\n",
    "\t\tif dropout is not None:\n",
    "\t\t\ttmp = layers.Dropout(dropout, name=f\"dropout_{i}\")(tmp)\n",
    "\n",
    "\t# temporäre Ausgabelayer des Personenstroms (n_pedestrians_towards & n_pedestrians_away)\n",
    "\ttmp = layers.Dense(2, activation=\"relu\", name=\"towards_away\")(tmp)\n",
    "\n",
    "\t# Summieren von n_pedestrians_towards & n_pedestrians_away, um n_pedestrians zu berechnen\n",
    "\tsum = layers.Dense(1, activation=\"relu\", name=\"sum\", kernel_initializer=initializers.Constant(1.0), bias_initializer=initializers.Zeros(), trainable=False)(tmp)\n",
    "\t\n",
    "\t# Aneinanderketten von n_pedestrians, n_pedestrians_towards und n_pedestrians_away (Output Layer)\n",
    "\tconcat = layers.Concatenate(name=\"output\")([sum, tmp])\n",
    "\n",
    "\t# Modell anlegen und kompilieren\n",
    "\tmodel = keras.Model(inputs=input, outputs=concat)\n",
    "\tmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\treturn model\n",
    "\n",
    "# Anlegen einer 10-fachen Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# Funktion zum Erstellen des Modells innerhalb der Cross Validation\n",
    "def model_build_fn(n_nodes, n_hidden_layers, dropout):\n",
    "\treturn build_NN(X=X, y=y, n_nodes=n_nodes,\n",
    "\t\t\t\t\tn_hidden_layers=n_hidden_layers, dropout=dropout)\n",
    "\n",
    "\n",
    "\n",
    "# Aufstellen des HPT mit entsprechenden Wertebereichen\n",
    "param_grid = {\n",
    "\t\"model__n_nodes\": [300, 400],\n",
    "\t\"model__n_hidden_layers\": [2, 3, 4],\n",
    "\t\"model__dropout\": [None, 0.2]\n",
    "}\n",
    "\n",
    "# Erstellen des GridSearchs\n",
    "modelCV = KerasRegressor(model=model_build_fn, batch_size=100, epochs=100, verbose=0)\n",
    "gs = GridSearchCV(estimator=modelCV, param_grid=param_grid, n_jobs=32, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "gs_result = gs.fit(X=X, y=y, verbose=1)\n",
    "\n",
    "\n",
    "# Ausgabe der besten Parameter und den dazugehörigen Fehlerwerten\n",
    "print(\"Best parameters set found on validation set:\")\n",
    "print()\n",
    "print(gs.best_params_)\n",
    "print()\n",
    "print(\"Score:\")\n",
    "print()\n",
    "print(f\"mse: {abs(gs.best_score_)}\")\n",
    "print(f\"rmse: {np.sqrt(abs(gs.best_score_))}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbildung der Vorhersagegenauigkeit für eine Woche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def score(model, X, y):\n",
    "\ty_pred = model.predict(X)\n",
    "\tscore = mean_squared_error(y, y_pred)\n",
    "\tx = np.arange(0, y_pred.shape[0])\n",
    "\tfor (i, name) in enumerate([\"ped\", \"ped_towards\", \"ped_away\"]):\n",
    "\t\tax = plt.subplot(3, 1, i+1)\n",
    "\t\tax.plot(x[0:24*7*1], np.array(y)[0:24*7*1,i], label=f\"realität_{name}\")\n",
    "\t\tax.plot(x[0:24*7*1], y_pred[0:24*7*1,i], label=f\"vorhersage_{name}\")\n",
    "\t\tax.legend()\n",
    "\tplt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "model = model_build_fn(n_nodes=400, n_hidden_layers=4, dropout=None)\n",
    "model.fit(x=X_train, y=y_train, batch_size=100, epochs=100, validation_data=(X_test, y_test))\n",
    "score(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufstellen des Modells mithilfe eines neuronalen Netzes (Standardkategorien + Berücksichtigung der Feiertage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import initializers\n",
    "\n",
    "# Eingangs- und Zielvariablen definieren\n",
    "feature_columns = [\"streetname\", \"hour\", \"weekday\", \"incidents\", \"weather_condition\", \"temperature\", \"year\", \"month\", \"day\", \"is_feiertag\"]\n",
    "target_columns = [\"n_pedestrians\", \"n_pedestrians_towards\", \"n_pedestrians_away\"]\n",
    "\n",
    "\n",
    "\n",
    "# Encoder anlegen und an Daten anpassen\n",
    "def fit_preprocess(X: pd.DataFrame, ordinal_features, onehot_features, numerical_features):\n",
    "\tX = X.copy()\n",
    "\n",
    "\tordinal_encoder = None\n",
    "\tif ordinal_features is not None and ordinal_features:\n",
    "\t\tordinal_encoder = OrdinalEncoder()\n",
    "\t\tX[ordinal_features] = ordinal_encoder.fit_transform(X[ordinal_features])\n",
    "\n",
    "\tonehot_encoder = None\n",
    "\tif onehot_features is not None and onehot_features:\n",
    "\t\tonehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\t\tencoded = onehot_encoder.fit_transform(X[onehot_features])\n",
    "\t\tencoded_onehot_columns = onehot_encoder.get_feature_names_out()\n",
    "\t\tX[encoded_onehot_columns] = pd.DataFrame(encoded, columns=encoded_onehot_columns)\n",
    "\n",
    "\tnumerical_encoder = None\n",
    "\tif numerical_features is not None and numerical_features:\n",
    "\t\tnumerical_encoder = StandardScaler()\n",
    "\t\tX[numerical_features] = numerical_encoder.fit_transform(X[numerical_features])\n",
    "\n",
    "\treturn ordinal_encoder, onehot_encoder, numerical_encoder\n",
    "\n",
    "\n",
    "# Codiert die Messkategorien\n",
    "def preprocess(X: pd.DataFrame, \n",
    "\t\t\t   ordinal_features, ordinal_encoder: OrdinalEncoder, \n",
    "\t\t\t   onehot_features, onehot_encoder: OneHotEncoder, \n",
    "\t\t\t   numerical_features, numerical_encoder):\n",
    "\tX = X.copy()\n",
    "\tfeatures = []\n",
    "\n",
    "\tif ordinal_encoder is not None:\n",
    "\t\tX[ordinal_features] = ordinal_encoder.transform(X[ordinal_features])\n",
    "\t\tfeatures.extend(ordinal_features)\n",
    "\n",
    "\tif onehot_encoder is not None:\n",
    "\t\tencoded = onehot_encoder.transform(X[onehot_features])\n",
    "\t\tencoded_onehot_columns = onehot_encoder.get_feature_names_out()\n",
    "\t\tX[encoded_onehot_columns] = pd.DataFrame(encoded, columns=encoded_onehot_columns)\n",
    "\t\tfeatures.extend(encoded_onehot_columns)\n",
    "\n",
    "\tif numerical_features is not None:\n",
    "\t\tX[numerical_features] = numerical_encoder.transform(X[numerical_features])\n",
    "\t\tfeatures.extend(numerical_features)\n",
    "\n",
    "\treturn X[features]\n",
    "\n",
    "\n",
    "# Kategorische und numerische Features den Codierungsverfahren zuordnen\n",
    "numerical_features = [\"year\", \"temperature\", \"is_feiertag\"]\n",
    "ordinal_features = [\"weather_condition\", \"incidents\"]\n",
    "onehot_features = [\"streetname\", \"weekday\", \"month\", \"hour\", \"day\"]\n",
    "\n",
    "\n",
    "# Aufstellen der Encoder für den Datensatz\n",
    "ordinal_encoder, onehot_encoder, numerical_encoder = fit_preprocess(datensatz_f, ordinal_features=ordinal_features,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t onehot_features=onehot_features,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t numerical_features=numerical_features)\n",
    "\n",
    "\n",
    "# Daten vorverarbeiten (codieren)\n",
    "X = preprocess(datensatz_f[feature_columns],\n",
    "\t\t\t\t\tordinal_features=ordinal_features,\n",
    "\t\t\t\t\tordinal_encoder=ordinal_encoder,\n",
    "\t\t\t\t\tonehot_features=onehot_features,\n",
    "\t\t\t\t\tonehot_encoder=onehot_encoder,\n",
    "\t\t\t\t\tnumerical_features=numerical_features,\n",
    "\t\t\t\t\tnumerical_encoder=numerical_encoder)\n",
    "y = datensatz_f[target_columns]\n",
    "\n",
    "\n",
    "\n",
    "# Neuronales Netzwerk definieren\n",
    "def build_NN(X, y, n_nodes, n_hidden_layers, dropout):\n",
    "\t# Input Layer anlegen\n",
    "\tinput = layers.Input(shape=(X.shape[1],), name=\"input\")\n",
    "\n",
    "\t# Hidden Layer anlegen mit möglichem Dropout\n",
    "\ttmp = input\n",
    "\tfor i in range(n_hidden_layers):\n",
    "\t\ttmp = layers.Dense(n_nodes, activation=\"relu\", name=f\"hidden_{i}\")(tmp)\n",
    "\t\tif dropout is not None:\n",
    "\t\t\ttmp = layers.Dropout(dropout, name=f\"dropout_{i}\")(tmp)\n",
    "\n",
    "\t# temporäre Ausgabelayer des Personenstroms (n_pedestrians_towards & n_pedestrians_away)\n",
    "\ttmp = layers.Dense(2, activation=\"relu\", name=\"towards_away\")(tmp)\n",
    "\n",
    "\t# Summieren von n_pedestrians_towards & n_pedestrians_away, um n_pedestrians zu berechnen\n",
    "\tsum = layers.Dense(1, activation=\"relu\", name=\"sum\", kernel_initializer=initializers.Constant(1.0), bias_initializer=initializers.Zeros(), trainable=False)(tmp)\n",
    "\t\n",
    "\t# Aneinanderketten von n_pedestrians, n_pedestrians_towards und n_pedestrians_away (Output Layer)\n",
    "\tconcat = layers.Concatenate(name=\"output\")([sum, tmp])\n",
    "\n",
    "\t# Modell anlegen und kompilieren\n",
    "\tmodel = keras.Model(inputs=input, outputs=concat)\n",
    "\tmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\treturn model\n",
    "\n",
    "# Anlegen einer 10-fachen Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# Funktion zum Erstellen des Modells innerhalb der Cross Validation\n",
    "def model_build_fn(n_nodes, n_hidden_layers, dropout):\n",
    "\treturn build_NN(X=X, y=y, n_nodes=n_nodes,\n",
    "\t\t\t\t\tn_hidden_layers=n_hidden_layers, dropout=dropout)\n",
    "\n",
    "\n",
    "\n",
    "# Aufstellen des HPT mit entsprechenden Wertebereichen\n",
    "param_grid = {\n",
    "\t\"model__n_nodes\": [400],\n",
    "\t\"model__n_hidden_layers\": [4],\n",
    "\t\"model__dropout\": [None]\n",
    "}\n",
    "\n",
    "\n",
    "# Erstellen des GridSearchs\n",
    "modelCV = KerasRegressor(model=model_build_fn, batch_size=100, epochs=100, verbose=0)\n",
    "gs = GridSearchCV(estimator=modelCV, param_grid=param_grid, n_jobs=32, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "gs_result = gs.fit(X=X, y=y, verbose=1)\n",
    "\n",
    "\n",
    "# Ausgabe der besten Parameter und den dazugehörigen Fehlerwerten\n",
    "print(\"Best parameters set found on validation set:\")\n",
    "print()\n",
    "print(gs.best_params_)\n",
    "print()\n",
    "print(\"Score:\")\n",
    "print()\n",
    "print(f\"mse: {abs(gs.best_score_)}\")\n",
    "print(f\"rmse: {np.sqrt(abs(gs.best_score_))}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
