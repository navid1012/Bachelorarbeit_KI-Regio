{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren, aller benötigten Packages und einlesen der Trainings-, Test- und Überprüfungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datensatz = pd.read_csv(\"./foot-traffic-wue/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten nach Fehlern und unrelevanten Daten bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datensatz_f = datensatz[\n",
    "    ~datensatz['incidents'].isin(['laser_failure'])\n",
    "    ]\n",
    "\n",
    "datensatz_f.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markierung der Feiertage im Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Feiertage\n",
    "feiertage = [\n",
    "    '2019-04-19', '2019-04-22', '2019-05-01', '2019-05-30', '2019-06-10', '2019-06-20', '2019-08-15',\n",
    "    '2019-10-03', '2019-11-01', '2019-12-25', '2019-12-26', '2020-01-01', '2020-01-06', '2020-04-10',\n",
    "    '2020-04-13', '2020-05-01', '2020-05-21', '2020-06-01', '2020-06-11', '2020-08-15', '2020-10-03',\n",
    "    '2020-11-01', '2020-12-25', '2020-12-26', '2021-01-01', '2021-01-06', '2021-04-02', '2021-04-05',\n",
    "    '2021-05-01', '2021-05-13', '2021-05-24', '2021-06-03', '2021-08-15', '2021-10-03', '2021-11-01',\n",
    "    '2021-12-25', '2021-12-26', '2022-01-01', '2022-01-06', '2022-04-15', '2022-04-18', '2022-05-01',\n",
    "    '2022-05-26', '2022-06-06', '2022-06-16', '2022-08-15', '2022-10-03', '2022-11-01', '2022-12-25',\n",
    "    '2022-12-26', '2023-01-01', '2023-01-06', '2023-04-07', '2023-04-10', '2023-05-01', '2023-05-18',\n",
    "    '2023-05-29', '2023-06-08', '2023-08-15', '2023-10-03', '2023-11-01', '2023-12-25', '2023-12-26',\n",
    "    '2024-01-01', '2024-01-06', '2024-03-29', '2024-04-01', '2024-05-01', '2024-05-09', '2024-05-20',\n",
    "    '2024-05-30', '2024-08-15'\n",
    "]\n",
    "\n",
    "# Konvertiere die Feiertage zu einem Set für eine schnellere Suche\n",
    "feiertage_set = set(feiertage)\n",
    "\n",
    "# Erstelle die neue Spalte \"is_feiertag\"\n",
    "datensatz_f['is_feiertag'] = datensatz_f['date'].apply(lambda x: 1 if x in feiertage_set else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilung der Spalte date in year, month und day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere die Spalte 'date' in ein Datetime-Format\n",
    "datensatz_f['date'] = pd.to_datetime(datensatz_f['date'])\n",
    "\n",
    "# Zerlege die Spalte in 'year', 'month' und 'day'\n",
    "datensatz_f['year'] = datensatz_f['date'].dt.year\n",
    "datensatz_f['month'] = datensatz_f['date'].dt.month\n",
    "datensatz_f['day'] = datensatz_f['date'].dt.day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusammenführen der Wetterbedingungen, welche in Tag und Nacht unterteilt sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Werte zusammenführen\n",
    "datensatz_f['weather_condition'] = datensatz_f['weather_condition'].replace({\n",
    "    'partly-cloudy-day': 'partly-cloudy',\n",
    "    'partly-cloudy-night': 'partly-cloudy',\n",
    "    'clear-day': 'clear',\n",
    "    'clear-night': 'clear'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufstellen des Modells mithilfe von XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Features und Zielvariablen definieren\n",
    "feature_columns = [\"streetname\", \"hour\", \"weekday\", \"incidents\", \"weather_condition\", \"temperature\", \"year\", \"month\", \"day\"]\n",
    "target_columns = [\"n_pedestrians\", \"n_pedestrians_towards\", \"n_pedestrians_away\"]\n",
    "\n",
    "X = datensatz_f[feature_columns]\n",
    "Y = datensatz_f[target_columns]\n",
    "\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Kategoriale Features kodieren\n",
    "categorical_columns = [\"streetname\", \"weekday\", \"weather_condition\", \"incidents\"]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X_categorical_encoded = encoder.fit_transform(X[categorical_columns])\n",
    "X_categorical_encoded_df = pd.DataFrame(X_categorical_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Numerische Features hinzufügen\n",
    "X_encoded = pd.concat([X_categorical_encoded_df, X[[\"hour\", \"temperature\", \"year\", \"month\", \"day\"]]], axis=1)\n",
    "X_columns = X_encoded.columns\n",
    "\n",
    "# 10-fache Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "rmse_scores = []\n",
    "mse_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_encoded):\n",
    "    X_train, X_test = X_encoded.iloc[train_index], X_encoded.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    # XGBoost-Modell einstellen der Hyperparameter\n",
    "    xgb_model = XGBRegressor(n_estimators=4000, max_depth=8, learning_rate=0.1, n_jobs=32, cv=kf)\n",
    "    \n",
    "    # Modell trainieren\n",
    "    xgb_model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Vorhersagen treffen\n",
    "    predictions = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Fehler berechnen\n",
    "    mse = mean_squared_error(Y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    mse_scores.append(mse)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# Durchschnittlichen MSE und RMSE berechnen\n",
    "mean_rmse = np.mean(rmse_scores)\n",
    "mean_mse = np.mean(mse_scores)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(f\"Durchschnittlicher RMSE über alle 10 Folds: {mean_rmse}\")\n",
    "print(f\"Durchschnittlicher MSE über alle 10 Folds: {mean_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning mit GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# nicht numerische Kategorien festlegen\n",
    "categorical_columns = [\"streetname\", \"weekday\", \"weather_condition\", \"incidents\"]\n",
    "\n",
    "# Codieren der categorical_features mit einem OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "X[categorical_columns] = encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "# 10-fache Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "#Wertebereich des HPT aufstellen\n",
    "param_grid = {\n",
    "\t\"learning_rate\"\t: [0.1, 0.12, 0.2],\n",
    "\t\"max_depth\"\t\t: [8, 10, 15, 20],\n",
    "\t\"n_estimators\"\t: [1000, 2000, 4000],\n",
    "}\n",
    "\n",
    "# HPT aufstellen\n",
    "gs = GridSearchCV(estimator=XGBRegressor(), param_grid=param_grid, n_jobs=32, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "gs_result = gs.fit(X=X, y=Y)\n",
    "\n",
    "# Ausgabe der besten Parameter\n",
    "print(\"Best parameters set found on validation set:\")\n",
    "print()\n",
    "print(gs.best_params_)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
